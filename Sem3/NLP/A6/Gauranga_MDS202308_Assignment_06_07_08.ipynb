{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 9690850,
          "sourceType": "datasetVersion",
          "datasetId": 5924616
        }
      ],
      "dockerImageVersionId": 30787,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "# Assignment 6, 7, & 8 - Abstract Generation from COVID-19 Dataset   \n",
        "### Name: Gauranga Kumar Baishya\n",
        "### Roll No.: MDS202325  \n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "kjY8UDNXe3dq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n"
      ],
      "metadata": {
        "id": "Td3jVucldG1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Importing Necessary Libraries"
      ],
      "metadata": {
        "id": "3K6Ir9khgIsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "2dxUwyo3gIGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Read extracted abstracts stored in form of text separated by lines"
      ],
      "metadata": {
        "id": "mYWO2AymgP8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Assgn678/extracted_abstracts.txt', 'r') as file:\n",
        "    text = file.readlines()\n",
        "    text = [line.strip() for line in text if len(line) > 50]\n",
        "\n",
        "# Training on first 60,000 (total approx 70,000 lines as some files have more than 1 abstracts) and keeping other for generation\n",
        "text = text[:60000]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:52:50.266062Z",
          "iopub.execute_input": "2024-11-16T07:52:50.266632Z",
          "iopub.status.idle": "2024-11-16T07:52:55.778405Z",
          "shell.execute_reply.started": "2024-11-16T07:52:50.266586Z",
          "shell.execute_reply": "2024-11-16T07:52:55.777554Z"
        },
        "id": "tkA3xePkTP8D"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Tokenize, prepare dataset and create vocabulary"
      ],
      "metadata": {
        "id": "CD2uFH2ehCU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    return text.split()\n",
        "\n",
        "tokenized_data = [preprocess_text(abstract) for abstract in text]\n",
        "\n",
        "# Count word frequencies\n",
        "all_words = [word for abstract in tokenized_data for word in abstract]\n",
        "word_counts = Counter(all_words)\n",
        "\n",
        "# Special tokens\n",
        "start_token, end_token, pad_token, unk_token = \"<START>\", \"<END>\", \"<PAD>\", \"<UNK>\"\n",
        "vocab_spcl = {pad_token: 0, start_token: 1, end_token: 2, unk_token: 3}\n",
        "\n",
        "# Create vocabulary\n",
        "most_common_words = word_counts.most_common()\n",
        "vocab = {word: idx + len(vocab_spcl) for idx, (word, _) in enumerate(most_common_words)}\n",
        "vocab.update(vocab_spcl)   # Added special tokens to original vocab\n",
        "\n",
        "id_to_word = {idx: wrd for wrd, idx in vocab.items()}"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:52:55.779605Z",
          "iopub.execute_input": "2024-11-16T07:52:55.780154Z",
          "iopub.status.idle": "2024-11-16T07:53:01.817694Z",
          "shell.execute_reply.started": "2024-11-16T07:52:55.780120Z",
          "shell.execute_reply": "2024-11-16T07:53:01.816862Z"
        },
        "id": "Dk6PNGtXTP8E"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Convert tokens to IDs and padding"
      ],
      "metadata": {
        "id": "gNiEwQInirw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokens_to_ids_with_start_end(tokens, vocab, start_token, end_token, unk_token):\n",
        "    return [vocab[start_token]] + [vocab.get(word, vocab[unk_token]) for word in tokens] + [vocab[end_token]]\n",
        "\n",
        "tokenized_data_ids = [tokens_to_ids_with_start_end(abstract, vocab, start_token, end_token, unk_token) for abstract in tokenized_data]\n",
        "\n",
        "max_len = 256\n",
        "padded_data_ids = pad_sequences(tokenized_data_ids, maxlen=max_len, padding='post', truncating='post', value=vocab[pad_token])"
      ],
      "metadata": {
        "id": "sQY_-7NPgs2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "18tdRhOljkA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class COVIDAbstractDataset(Dataset):\n",
        "    def __init__(self, tokenized_data):\n",
        "        self.data = tokenized_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.data[idx][:-1]), torch.tensor(self.data[idx][1:])\n",
        "\n",
        "def collate_fn(batch):\n",
        "    inputs, targets = zip(*batch)\n",
        "    inputs = torch.stack(inputs, dim=0)\n",
        "    targets = torch.stack(targets, dim=0)\n",
        "    return inputs, targets\n",
        "\n",
        "# Split data into training and validation sets\n",
        "train_data, val_data = train_test_split(padded_data_ids, test_size=0.25, random_state=42)\n",
        "\n",
        "batch_size = 8\n",
        "train_dataset = COVIDAbstractDataset(train_data)\n",
        "val_dataset = COVIDAbstractDataset(val_data)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:53:01.818810Z",
          "iopub.execute_input": "2024-11-16T07:53:01.819141Z",
          "iopub.status.idle": "2024-11-16T07:53:01.857310Z",
          "shell.execute_reply.started": "2024-11-16T07:53:01.819077Z",
          "shell.execute_reply": "2024-11-16T07:53:01.856502Z"
        },
        "id": "FPnFPy9kTP8G"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment - 06"
      ],
      "metadata": {
        "id": "Y4Vmu7PYjkup"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Description of the architecture\n",
        "\n",
        "It consists of three primary components: an **Embedding Layer**, a **Stacked LSTM Layer**, and a **Fully Connected (Linear) Layer**.\n",
        "\n",
        "#### **1. Embedding Layer**\n",
        "- **Purpose**: Maps each token to a dense vector representation, capturing semantic relationships.\n",
        "- **Input**: Sequence of token indices.\n",
        "- **Output**: Each token is embedded into a **128-dimensional vector**.\n",
        "- **Details**: Vocabulary size \\( V = 324,033 \\), output embedding size = **128**.\n",
        "\n",
        "#### **2. LSTM Layer**\n",
        "- **Purpose**: Captures temporal dependencies between tokens, learning contextual relationships.\n",
        "- **Input**: Embedded token sequence (128-dimensional vectors).\n",
        "- **Hidden State**: *2 stacked LSTM layers* with **256-dimensional hidden states**.\n",
        "- **Output**: Sequence of hidden states used for prediction.\n",
        "\n",
        "#### **3. Fully Connected (Linear) Layer**\n",
        "- **Purpose**: Maps the final LSTM hidden state to a probability distribution over the vocabulary.\n",
        "- **Input**: 256-dimensional hidden state from the LSTM.\n",
        "- **Output**: **324,033**-dimensional vector representing the predicted token probabilities.\n",
        "\n",
        "#### **4. Hidden State Initialization**\n",
        "- The LSTMâ€™s hidden and cell states are initialized as zero vectors of shape **(2, batch_size, 256)**.\n",
        "\n",
        "#### **Forward Pass**\n",
        "1. **Input**: Sequence of token indices.\n",
        "2. **Embedding**: Tokens are converted into 128-dimensional vectors.\n",
        "3. **LSTM**: The embedded sequence is processed by the stacked LSTM layers to capture sequential dependencies.\n",
        "4. **Prediction**: The final hidden state is passed through the fully connected layer to generate a probability distribution over the vocabulary.\n"
      ],
      "metadata": {
        "id": "geuBiTJ3q3IQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 128\n",
        "hidden_dim = 256\n",
        "num_layers = 2\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "class AbstractGenerator(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
        "        super(AbstractGenerator, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        embedded = self.embedding(x)\n",
        "        out, hidden = self.lstm(embedded, hidden)\n",
        "        out = self.fc(out)\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return (torch.zeros(num_layers, batch_size, hidden_dim).to(device),\n",
        "                torch.zeros(num_layers, batch_size, hidden_dim).to(device))\n",
        "\n",
        "model = AbstractGenerator(vocab_size, embedding_dim, hidden_dim, num_layers).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.005)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:53:01.860946Z",
          "iopub.execute_input": "2024-11-16T07:53:01.861407Z",
          "iopub.status.idle": "2024-11-16T07:53:03.896682Z",
          "shell.execute_reply.started": "2024-11-16T07:53:01.861369Z",
          "shell.execute_reply": "2024-11-16T07:53:03.895759Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12Ruz1u-TP8H",
        "outputId": "13b6dc5d-9876-4e54-dfaa-43ffb791e62b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASbglmBgpDRW",
        "outputId": "ac548a91-b002-497f-afab-8f315c677e01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AbstractGenerator(\n",
              "  (embedding): Embedding(324033, 128)\n",
              "  (lstm): LSTM(128, 256, num_layers=2, batch_first=True)\n",
              "  (fc): Linear(in_features=256, out_features=324033, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oiK5uzKjjsHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:53:03.920462Z",
          "iopub.execute_input": "2024-11-16T07:53:03.920756Z",
          "iopub.status.idle": "2024-11-16T07:53:03.928796Z",
          "shell.execute_reply.started": "2024-11-16T07:53:03.920724Z",
          "shell.execute_reply": "2024-11-16T07:53:03.927848Z"
        },
        "id": "Wrk3yCHbTP8J"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ja410WlSjsxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "num_epochs = 20\n",
        "loss_history = {'train': [], 'val': []}\n",
        "best_val_loss = float('inf')\n",
        "patience = 5\n",
        "epochs_without_improvement = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_pbar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
        "\n",
        "    for inputs, targets in train_pbar:\n",
        "        actual_batch_size = inputs.size(0)\n",
        "        hidden = model.init_hidden(actual_batch_size)\n",
        "\n",
        "        inputs, targets = inputs.to(device), targets.to(device).long()  # Ensure targets are torch.long\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs, hidden = model(inputs, hidden)\n",
        "        hidden = tuple(h.detach() for h in hidden)\n",
        "\n",
        "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        train_pbar.set_postfix({\"Batch Loss\": loss.item()})\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    loss_history['train'].append(train_loss)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_pbar = tqdm(val_loader, desc=\"Validation\", leave=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_pbar:\n",
        "            actual_batch_size = inputs.size(0)\n",
        "            hidden = model.init_hidden(actual_batch_size)\n",
        "\n",
        "            inputs, targets = inputs.to(device), targets.to(device).long()  # Ensure targets are torch.long\n",
        "            outputs, hidden = model(inputs, hidden)\n",
        "            loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            val_pbar.set_postfix({\"Batch Loss\": loss.item()})\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    loss_history['val'].append(val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} Results: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        epochs_without_improvement = 0\n",
        "        print(\"Validation loss improved. Saving the model...\")\n",
        "        torch.save(model.state_dict(), f\"best_model_epoch_{epoch+1}.pth\")\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "        print(\"Validation loss did not improve.\")\n",
        "\n",
        "    if epochs_without_improvement >= patience:\n",
        "        print(f\"Early stopping at epoch {epoch+1}\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7_ecZ7ZWXkc",
        "outputId": "54fc231c-575b-477b-c343-1de64890fd93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "                                                                                \n",
            "Epoch 1/20 Results: Train Loss: 3.1630, Val Loss: 2.8533\n",
            "Validation loss improved. Saving the model...\n",
            "\n",
            "Epoch 2/20\n",
            "                                                                                \n",
            "Epoch 2/20 Results: Train Loss: 2.6793, Val Loss: 2.7405\n",
            "Validation loss improved. Saving the model...\n",
            "\n",
            "Epoch 3/20\n",
            "                                                                                \n",
            "Epoch 3/20 Results: Train Loss: 2.4599, Val Loss: 2.7131\n",
            "Validation loss improved. Saving the model...\n",
            "\n",
            "Epoch 4/20\n",
            "                                                                                \n",
            "Epoch 4/20 Results: Train Loss: 2.2929, Val Loss: 2.7466\n",
            "Validation loss did not improve.\n",
            "\n",
            "Epoch 5/20\n",
            "                                                                                \n",
            "Epoch 5/20 Results: Train Loss: 2.1621, Val Loss: 2.8026\n",
            "Validation loss did not improve.\n",
            "\n",
            "Epoch 6/20\n",
            "                                                                                \n",
            "Epoch 6/20 Results: Train Loss: 2.0640, Val Loss: 2.8520\n",
            "Validation loss did not improve.\n",
            "\n",
            "Epoch 7/20\n",
            "                                                                                \n",
            "Epoch 7/20 Results: Train Loss: 1.9959, Val Loss: 2.9002\n",
            "Validation loss did not improve.\n",
            "\n",
            "Epoch 8/20\n",
            "                                                                                \n",
            "Epoch 8/20 Results: Train Loss: 1.9455, Val Loss: 2.9471\n",
            "Validation loss did not improve.\n",
            "Early stopping at epoch 8\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss_history['train'], label='Train Loss')\n",
        "plt.plot(loss_history['val'], label='Validation Loss')\n",
        "plt.title(\"Loss over Epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "6pWMNyxgTP8K"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Save the model weights"
      ],
      "metadata": {
        "id": "_PzAPfB-jwGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/Assgn678/generator_LSTM_full_sent.pth\")\n",
        "print(\"Model weights saved!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zABrNHB_Zao6",
        "outputId": "0a6298d3-4b12-49f0-b284-c548853c1abf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model weights saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Load the model weights"
      ],
      "metadata": {
        "id": "waQErZ-Ujzf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " model.load_state_dict(torch.load(\"/content/drive/MyDrive/Assgn678/generator_LSTM_full_sent.pth\", weights_only=True))\n",
        "print(\"Model weights loaded!\")"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xq4HNN0TP8L",
        "outputId": "2830a2be-8344-495d-a4ed-82d01b0d8133"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model weights loaded!\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_to_token_ids(sentence, word_to_id, start_token=1):\n",
        "    tokens = sentence.split()\n",
        "    token_ids = [word_to_id.get(token, word_to_id['<UNK>']) for token in tokens]\n",
        "    token_ids = [start_token] + token_ids\n",
        "    return token_ids\n",
        "\n",
        "def generate_abstract_from_sentence(input_sentence, word_to_id, id_to_word, max_length=50, start_token=1, end_token=2):\n",
        "    model.eval()\n",
        "    input_tokens = sentence_to_token_ids(input_sentence, word_to_id, start_token)\n",
        "    input_tensor = torch.tensor(input_tokens).unsqueeze(0).to(device)\n",
        "    hidden = model.init_hidden(1)\n",
        "    generated_tokens = input_tokens\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        output, hidden = model(input_tensor, hidden)\n",
        "        next_token = output.squeeze(0)[-1].argmax(dim=-1).item()\n",
        "\n",
        "        generated_tokens.append(next_token)\n",
        "        input_tensor = torch.tensor([[next_token]]).to(device)\n",
        "\n",
        "        if next_token == end_token:\n",
        "            break\n",
        "\n",
        "    return ' '.join([id_to_word[token] for token in generated_tokens[1:]])"
      ],
      "metadata": {
        "trusted": true,
        "id": "CGSRyVMCTP8M"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sen1 = 'COVID-19 caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) primarily appeared in Wuhan, China, in December 2019. At present, no proper therapy and vaccinations are available for the disease, and it is increasing day by day with a high mortality rate. Pharmacophore based virtual screening of the selected natural product databases followed by Glide molecular docking and dynamics studies against SARS-CoV-2 main protease was investigated to identify potential ligands that may act as inhibitors. The molecules SN00293542 and SN00382835 revealed the highest docking score of 214.57 and 212.42 kcal/mol, respectively, when compared with the co-crystal ligands of PDB-6Y2F (O6K) and 6W63 (X77) of the SARS-CoV-2 M pro . To further validate the interactions of top scored molecules SN00293542 and SN00382835, molecular dynamics study of 100 ns was carried out. This indicated that the protein-ligand complex was stable throughout the simulation period, and minimal backbone fluctuations have ensued in the system. Post-MM-GBSA analysis of molecular dynamics data showed free binding energy-71.7004 1/2 7.98, 256.811/2 7.54 kcal/mol, respectively. The computational study identified several ligands that may act as potential inhibitors of SARS-CoV-2 M pro . The top-ranked molecules SN00293542, and SN00382835 occupied the active site of the target, the main protease like that of the co-crystal ligand. These molecules may emerge as a promising ligands against SARS-CoV-2 and thus needs further detailed investigations.'"
      ],
      "metadata": {
        "trusted": true,
        "id": "ox4bOcJuTP8M"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = 'COVID-19 caused by severe acute respiratory'\n",
        "generated_text = generate_abstract_from_sentence(input_sentence, vocab, id_to_word)\n",
        "print(\"Generated Abstract:\", generated_text)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oduQpNpVTP8N",
        "outputId": "c2890cea-8a6e-4848-c8e5-6d7229643828"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Abstract: <UNK> caused by severe acute respiratory syndrome coronavirus 2 (sars-cov-2) is a novel coronavirus named coronavirus disease 2019 caused by severe acute respiratory syndrome coronavirus 2 (sars-cov-2). the spike protein of sars-cov is a novel coronavirus named coronavirus disease 2019 caused by severe acute respiratory syndrome coronavirus 2 (sars-cov-2). the spike (s) protein binds to the\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "yWUyS2eJd5YC",
        "outputId": "32e8d4b6-706d-4567-cede-e93931876089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<UNK> caused by severe acute respiratory syndrome coronavirus 2 (sars-cov-2) is a novel coronavirus named coronavirus disease 2019 caused by severe acute respiratory syndrome coronavirus 2 (sars-cov-2). the spike protein of sars-cov is a novel coronavirus named coronavirus disease 2019 caused by severe acute respiratory syndrome coronavirus 2 (sars-cov-2). the spike (s) protein binds to the'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sen2 =  'Results: A cohort of 488 patients was analyzed. Infective causes were found in 137 (28.1%) patients. Bacterial, viral and mixed infections were detected in 86 (17.6%), 41 (8.4%) and 10 (2.0%) patients, respectively. Bacteriology was established mostly by sputum culture and virology by nasopharyngeal aspirate (NPA) viral culture. The commonest bacterial isolates were Haemophilus influenzae (31), Pseudomonas aeruginosa (15), Mycobacterium tuberculosis (14), Klebsiella spp. (9) and Streptococcus pneumoniae (6). Influenza A virus (28, 8 were pandemic 2009 A/H1N1 subtype) and respiratory syncytial virus (16) were the most frequent viral causes. Independent predictors of viral pneumonia included nursing home residence (RR 3.056, P = 0.009) and absence of leukocytosis (RR 0.425, P = 0.026).'"
      ],
      "metadata": {
        "trusted": true,
        "id": "3KO2QSL4TP8N"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = \"A cohort of 488 patients was analyzed. Infective causes\"\n",
        "generated_text = generate_abstract_from_sentence(input_sentence, vocab, id_to_word)\n",
        "print(\"Generated Abstract:\", generated_text)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXpwvVuBTP8O",
        "outputId": "1333bc00-cd0e-4b9f-93ba-f60fc0b47177"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Abstract: <UNK> cohort of 488 patients was analyzed. <UNK> causes severe hypoxia (pao2 50 mm mmol/l and a four-fold muscarinic cutoff value of the nose/eye were not associated with the covid-19 pandemic. <END>\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "t2XJBZdMd-cx",
        "outputId": "d92c7c40-8453-483a-80b4-0d984c22a35a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<UNK> cohort of 488 patients was analyzed. <UNK> causes severe hypoxia (pao2 50 mm mmol/l and a four-fold muscarinic cutoff value of the nose/eye were not associated with the covid-19 pandemic. <END>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sen3 = 'We have identified here the main venom proteins of two braconid wasps, Psyttalia lounsburyi (two strains from South Africa and Kenya) and P. concolor, olive fruit fly parasitoids that differ in host range. Among the shared abundant proteins, we found a GH1 Î²-glucosidase and a family of leucine-rich repeat (LRR) proteins. Olive is extremely rich in glycoside compounds that are hydrolyzed by Î²-glucosidases into defensive toxic products in response to phytophagous insect attacks. Assuming that Psyttalia host larvae sequester ingested glycosides, the injected venom GH1 Î²-glucosidase could induce the release of toxic compounds, thus participating in parasitism success by weakening the host. Venom LRR proteins are similar to truncated Toll-like receptors and may possibly scavenge the host immunity. The abundance of one of these LRR proteins in the venom of only one of the two P. lounsburyi strains evidences intraspecific variation in venom composition. Altogether, venom intra-and inter-specific variation in Psyttalia spp. were much lower than previously reported in the Leptopilina genus (Figitidae), suggesting it might depend upon the parasitoid taxa.'"
      ],
      "metadata": {
        "trusted": true,
        "id": "TgMZm2hCTP8O"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = 'We have identified here the main venom'\n",
        "generated_text = generate_abstract_from_sentence(input_sentence, vocab, id_to_word)\n",
        "print(\"Generated Abstract:\", generated_text)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmLvV-nATP8Q",
        "outputId": "52c8930f-b39e-482a-8ddd-93e9f0eeeda9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Abstract: <UNK> have identified here the main venom and g-quadruplex properties of the human coronavirus (hcov) 229e and the non-pathogenic mopeia virus (mopv) were isolated from the human body, and the hemagglutinin-esterase (he) protein was inserted into the hemolymph of the body, and the hemagglutinin-esterase (he) protein was inserted into the golgi group and the âˆ†na(rbd) of the\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "O9LPsLeleBRr",
        "outputId": "f675fc2c-e994-4f34-ab79-0fc5c6fb39ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<UNK> have identified here the main venom and g-quadruplex properties of the human coronavirus (hcov) 229e and the non-pathogenic mopeia virus (mopv) were isolated from the human body, and the hemagglutinin-esterase (he) protein was inserted into the hemolymph of the body, and the hemagglutinin-esterase (he) protein was inserted into the golgi group and the âˆ†na(rbd) of the'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = \"Background: Evidence on the effectiveness of respiratory\"\n",
        "generated_text = generate_abstract_from_sentence(input_sentence, vocab, id_to_word)\n",
        "print(\"Generated Abstract:\", generated_text)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V36l7CEqTP8Q",
        "outputId": "3bb687c2-70e5-4cee-84b5-3e8e2f9241b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Abstract: <UNK> <UNK> on the effectiveness of respiratory viral infections in the respiratory tract and pancreas of the mexican population are not fully known. the aim of this study was to investigate the association between respiratory signs/symptoms and respiratory viruses in children with acute respiratory distress syndrome (ards). <END>\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "cvqrWSFmeGf7",
        "outputId": "8cc9a87b-81de-472e-a09e-be62ce9f1ab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<UNK> <UNK> on the effectiveness of respiratory viral infections in the respiratory tract and pancreas of the mexican population are not fully known. the aim of this study was to investigate the association between respiratory signs/symptoms and respiratory viruses in children with acute respiratory distress syndrome (ards). <END>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RyvF9Z1neHDH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}