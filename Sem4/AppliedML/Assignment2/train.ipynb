{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8fa59e5",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a049409b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn import metrics\n",
    "import random\n",
    "random.seed(42)\n",
    "from urllib.parse import urlparse\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.metrics import auc, precision_recall_curve\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc7b15e",
   "metadata": {},
   "source": [
    "### Loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c8bfce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "val = pd.read_csv(\"validation.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b552af43-9e2b-465d-93f8-7a726a4ae874",
   "metadata": {},
   "source": [
    "### Preparing the attributes and labels for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a44ba580-2f4f-46bb-ae9a-5c5f01c8a3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train[\"processed_text\"], train[\"spam\"]\n",
    "X_val, y_val = val[\"processed_text\"], val[\"spam\"]\n",
    "X_test, y_test = test[\"processed_text\"], test[\"spam\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efe0b38-4720-47ed-9f42-2637bbf2585c",
   "metadata": {},
   "source": [
    "### Some predefined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15b8b6cb-3e33-41d5-88e7-20db3749d824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, y_test):\n",
    "    predictions = np.array(predictions)\n",
    "    y_test = np.array(y_test)\n",
    "    TP = np.sum((predictions == 1)[y_test==1])\n",
    "    TN = np.sum((predictions == 0)[y_test==0])\n",
    "    FP = np.sum((predictions == 1)[y_test == 0])\n",
    "    FN = np.sum((predictions == 0)[y_test == 1])\n",
    "    accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "    return accuracy\n",
    "\n",
    "def precision(predictions, y_test):\n",
    "    predictions = np.array(predictions)\n",
    "    y_test = np.array(y_test)\n",
    "    TP = np.sum((predictions == 1)[y_test==1])\n",
    "    FP = np.sum((predictions == 1)[y_test == 0])\n",
    "    precision = TP/(TP + FP)\n",
    "    return precision\n",
    "\n",
    "def recall(predictions, y_test):\n",
    "    predictions = np.array(predictions)\n",
    "    y_test = np.array(y_test)\n",
    "    TP = np.sum((predictions == 1)[y_test==1])\n",
    "    FN = np.sum((predictions == 0)[y_test == 1])\n",
    "    recall = TP/(TP+FN)\n",
    "    return recall\n",
    "\n",
    "def f1_score(predictions, y_test):\n",
    "    predictions = np.array(predictions)\n",
    "    y_test = np.array(y_test)\n",
    "    TP = np.sum((predictions == 1)[y_test==1])\n",
    "    FP = np.sum((predictions == 1)[y_test == 0])\n",
    "    FN = np.sum((predictions == 0)[y_test==1])\n",
    "    precision = TP/(TP + FP)\n",
    "    recall = TP/(TP + FN)\n",
    "    if precision == 0 and recall == 0:\n",
    "        return 0\n",
    "    f1 = 2 * precision * recall/(precision + recall)\n",
    "    return f1\n",
    "\n",
    "def AUCPR(predictions, y_test):\n",
    "    precision, recall, _ = precision_recall_curve(y_test, predictions)\n",
    "    return auc(recall, precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd74ba3-4db2-4a75-b7b4-d0abca6189e2",
   "metadata": {},
   "source": [
    "### Training the 3 benchmark models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1f08881-b338-4da1-8a1a-2361d3df561d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes\n",
      "\n",
      "\n",
      "On validation Dataset:\n",
      "Accuracy : 98.72%\n",
      "Precision : 98.1%\n",
      "Recall : 96.73%\n",
      "f1 score : 97.41%\n",
      "AUCPR : 97.82%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Logistic Regression\n",
      "\n",
      "\n",
      "On validation Dataset:\n",
      "Accuracy : 98.49%\n",
      "Precision : 97.63%\n",
      "Recall : 96.26%\n",
      "f1 score : 96.94%\n",
      "AUCPR : 97.41%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Random Forest Classifier\n",
      "\n",
      "\n",
      "On validation Dataset:\n",
      "Accuracy : 94.99%\n",
      "Precision : 100.0%\n",
      "Recall : 79.91%\n",
      "f1 score : 88.83%\n",
      "AUCPR : 92.46%\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "print(\"Naive Bayes\\n\\n\")\n",
    "pipeline_nb = make_pipeline(CountVectorizer(), MultinomialNB(alpha = 0.1))\n",
    "pipeline_nb.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on validation set\n",
    "predictions = pipeline_nb.predict(X_val)\n",
    "print(\"On validation Dataset:\", end = \"\\n\")\n",
    "print(\"Accuracy : \" + str(round(accuracy(predictions, y_val)*100, 2)) + \"%\")\n",
    "print(\"Precision : \" + str(round(precision(predictions, y_val)*100, 2)) + \"%\")\n",
    "print(\"Recall : \" + str(round(recall(predictions, y_val)*100, 2)) + \"%\")\n",
    "print(\"f1 score : \" + str(round(f1_score(predictions, y_val)*100, 2)) + \"%\")\n",
    "print(\"AUCPR : \" + str(round(AUCPR(predictions, y_val)*100, 2)) + \"%\")\n",
    "print(\"\\n\\n\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Logistic Regression\n",
    "print(\"Logistic Regression\\n\\n\")\n",
    "pipeline_lr = make_pipeline(CountVectorizer(), LogisticRegression(random_state = 42))\n",
    "pipeline_lr.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on validation set\n",
    "predictions = pipeline_lr.predict(X_val)\n",
    "print(\"On validation Dataset:\", end = \"\\n\")\n",
    "print(\"Accuracy : \" + str(round(accuracy(predictions, y_val)*100, 2)) + \"%\")\n",
    "print(\"Precision : \" + str(round(precision(predictions, y_val)*100, 2)) + \"%\")\n",
    "print(\"Recall : \" + str(round(recall(predictions, y_val)*100, 2)) + \"%\")\n",
    "print(\"f1 score : \" + str(round(f1_score(predictions, y_val)*100, 2)) + \"%\")\n",
    "print(\"AUCPR : \" + str(round(AUCPR(predictions, y_val)*100, 2)) + \"%\")\n",
    "print(\"\\n\\n\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Random Forest Classifier\n",
    "print(\"Random Forest Classifier\\n\\n\")\n",
    "pipeline_rf = make_pipeline(CountVectorizer(), RandomForestClassifier(random_state = 42, max_depth=60, n_jobs=-1))\n",
    "pipeline_rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on validation set\n",
    "predictions = pipeline_rf.predict(X_val)\n",
    "print(\"On validation Dataset:\", end = \"\\n\")\n",
    "print(\"Accuracy : \" + str(round(accuracy(predictions, y_val)*100, 2)) + \"%\")\n",
    "print(\"Precision : \" + str(round(precision(predictions, y_val)*100, 2)) + \"%\")\n",
    "print(\"Recall : \" + str(round(recall(predictions, y_val)*100, 2)) + \"%\")\n",
    "print(\"f1 score : \" + str(round(f1_score(predictions, y_val)*100, 2)) + \"%\")\n",
    "print(\"AUCPR : \" + str(round(AUCPR(predictions, y_val)*100, 2)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae6e6ce-396f-405d-b151-b3599ab0d22b",
   "metadata": {},
   "source": [
    "### Logging models to mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d322f05-11c9-47de-bc87-8ee58d9f5a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'Naive Bayes model'.\n",
      "Created version '1' of model 'Naive Bayes model'.\n",
      "Successfully registered model 'Logistic Regression model'.\n",
      "Created version '1' of model 'Logistic Regression model'.\n",
      "Successfully registered model 'Random Forest model'.\n",
      "Created version '1' of model 'Random Forest model'.\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "with mlflow.start_run(run_name=\"Naive Bayes\"):\n",
    "    y_pred = pipeline_nb.predict(X_test)\n",
    "    mlflow.log_param(\"model_name\", \"Naive Bayes\")\n",
    "    mlflow.log_metric(\"accuracy\", accuracy(y_pred, y_test))\n",
    "    mlflow.log_metric(\"precision\", precision(y_pred, y_test))\n",
    "    mlflow.log_metric(\"recall\", recall(y_pred, y_test))\n",
    "    mlflow.log_metric(\"f1 score\", f1_score(y_pred, y_test))\n",
    "    mlflow.log_metric(\"AUCPR\", AUCPR(y_pred, y_test))\n",
    "    mlflow.log_dict(np.array(confusion_matrix(y_test, y_pred)).tolist(), \"confusion_matrix.json\")\n",
    "    mlflow.sklearn.log_model(pipeline_nb, \"model\")\n",
    "    \n",
    "    tracking_url_type = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=pipeline_nb,\n",
    "        artifact_path=\"sklearn-model\",\n",
    "        registered_model_name=\"Naive Bayes model\"\n",
    "    )\n",
    "    if tracking_url_type != \"file\":\n",
    "        mlflow.sklearn.log_model(pipeline_nb, \"model\", registered_model_name=\"Naive Bayes\")\n",
    "    else:\n",
    "        mlflow.sklearn.log_model(pipeline_nb, \"model\")\n",
    "\n",
    "\n",
    "# Logistic Regression\n",
    "with mlflow.start_run(run_name=\"Logistic Regression\"):\n",
    "    y_pred = pipeline_lr.predict(X_test)\n",
    "    mlflow.log_param(\"model_name\", \"Logistic Regression\")\n",
    "    mlflow.log_metric(\"accuracy\", accuracy(y_pred, y_test))\n",
    "    mlflow.log_metric(\"precision\", precision(y_pred, y_test))\n",
    "    mlflow.log_metric(\"recall\", recall(y_pred, y_test))\n",
    "    mlflow.log_metric(\"f1 score\", f1_score(y_pred, y_test))\n",
    "    mlflow.log_metric(\"AUCPR\", AUCPR(y_pred, y_test))\n",
    "    mlflow.log_dict(np.array(confusion_matrix(y_test, y_pred)).tolist(), \"confusion_matrix.json\")\n",
    "    mlflow.sklearn.log_model(pipeline_lr, \"model\")\n",
    "    \n",
    "    tracking_url_type = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=pipeline_nb,\n",
    "        artifact_path=\"sklearn-model\",\n",
    "        registered_model_name=\"Logistic Regression model\"\n",
    "    )\n",
    "    if tracking_url_type != \"file\":\n",
    "        mlflow.sklearn.log_model(pipeline_lr, \"model\", registered_model_name=\"Logistic Regression\")\n",
    "    else:\n",
    "        mlflow.sklearn.log_model(pipeline_lr, \"model\")\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "with mlflow.start_run(run_name=\"Random Forest\"):\n",
    "    y_pred = pipeline_rf.predict(X_test)\n",
    "    mlflow.log_param(\"model_name\", \"Random Forest\")\n",
    "    mlflow.log_metric(\"accuracy\", accuracy(y_pred, y_test))\n",
    "    mlflow.log_metric(\"precision\", precision(y_pred, y_test))\n",
    "    mlflow.log_metric(\"recall\", recall(y_pred, y_test))\n",
    "    mlflow.log_metric(\"f1 score\", f1_score(y_pred, y_test))\n",
    "    mlflow.log_metric(\"AUCPR\", AUCPR(y_pred, y_test))\n",
    "    mlflow.log_dict(np.array(confusion_matrix(y_test, y_pred)).tolist(), \"confusion_matrix.json\")\n",
    "    mlflow.sklearn.log_model(pipeline_rf, \"model\")\n",
    "    \n",
    "    tracking_url_type = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=pipeline_rf,\n",
    "        artifact_path=\"sklearn-model\",\n",
    "        registered_model_name=\"Random Forest model\"\n",
    "    )\n",
    "    if tracking_url_type != \"file\":\n",
    "        mlflow.sklearn.log_model(pipeline_rf, \"model\", registered_model_name=\"Random Forest\")\n",
    "    else:\n",
    "        mlflow.sklearn.log_model(pipeline_rf, \"model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
